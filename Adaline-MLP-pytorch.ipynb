{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW01_P5_P6_AlexOlson.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_sE_hQZeQfp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "plt.ion()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOy3wB1yRmxP",
        "colab_type": "text"
      },
      "source": [
        "**Problem 5**\n",
        "\n",
        "Part (a)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAMfyE8d5AOm",
        "colab_type": "code",
        "outputId": "ba485fbf-0805-493b-85f2-f26f6c690f20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        }
      },
      "source": [
        "w = torch.tensor([[[2., 2.],[2., 2.]],[[2., 2.],[2., 2.]]], requires_grad=True)\n",
        "print(f'\\nw: 2*2*2 tensor initialized to 2.0: \\n{w}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "w: 2*2*2 tensor initialized to 2.0: \n",
            "tensor([[[2., 2.],\n",
            "         [2., 2.]],\n",
            "\n",
            "        [[2., 2.],\n",
            "         [2., 2.]]], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXAhUdh0S_Ga",
        "colab_type": "text"
      },
      "source": [
        "Part (b)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voNB0KnaS-RI",
        "colab_type": "code",
        "outputId": "75b2a434-95fb-467e-b9ca-226149b58845",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        }
      },
      "source": [
        "x = w/3\n",
        "print(f'\\nx = w/3 \\n{x}')\n",
        "y = x.pow(3)\n",
        "print(f'\\ny = x^3 \\n{y}')\n",
        "z = y.sqrt()\n",
        "print(f'\\nz = sqrt(y) \\n{z}')\n",
        "out = z.mean()\n",
        "print(f'\\nout = mean(z) \\n{out}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "x = w/3 \n",
            "tensor([[[0.6667, 0.6667],\n",
            "         [0.6667, 0.6667]],\n",
            "\n",
            "        [[0.6667, 0.6667],\n",
            "         [0.6667, 0.6667]]], grad_fn=<DivBackward0>)\n",
            "\n",
            "y = x^3 \n",
            "tensor([[[0.2963, 0.2963],\n",
            "         [0.2963, 0.2963]],\n",
            "\n",
            "        [[0.2963, 0.2963],\n",
            "         [0.2963, 0.2963]]], grad_fn=<PowBackward0>)\n",
            "\n",
            "z = sqrt(y) \n",
            "tensor([[[0.5443, 0.5443],\n",
            "         [0.5443, 0.5443]],\n",
            "\n",
            "        [[0.5443, 0.5443],\n",
            "         [0.5443, 0.5443]]], grad_fn=<SqrtBackward>)\n",
            "\n",
            "out = mean(z) \n",
            "0.5443310737609863\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGE_PLKWTftD",
        "colab_type": "text"
      },
      "source": [
        "The gradient functions that are being tracked beginning with the output are:\n",
        "\n",
        "mean <- square root <- power$^3$ <- division\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6Vz1us5UILJ",
        "colab_type": "text"
      },
      "source": [
        "Part (c)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vboKx9vUJ_c",
        "colab_type": "code",
        "outputId": "3cb0d744-5e04-476a-c84d-e2067bd4ae39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        }
      },
      "source": [
        "out.backward()\n",
        "print(w.grad)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[0.0510, 0.0510],\n",
            "         [0.0510, 0.0510]],\n",
            "\n",
            "        [[0.0510, 0.0510],\n",
            "         [0.0510, 0.0510]]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUhF1oTeUV9Q",
        "colab_type": "text"
      },
      "source": [
        "Part (d)\n",
        "\n",
        "$f(w) = w^{\\frac{1}{2}} \\hspace{1cm} f'(w) = \\frac{1}{2\\sqrt{w}}$\n",
        "\n",
        "$g(w) = w^3 \\hspace{1cm} g'(w) = 3w^2$\n",
        "\n",
        "$h(w) = \\frac{1}{3}w \\hspace{1cm} h'(w) = \\frac{1}{3}$\n",
        "\n",
        "$\\frac{\\partial {out}}{\\partial{w_i}} |_{w_i=2} = (\\frac{1}{N}) \\frac{\\partial {}}{\\partial{w_i}} |_{w_i=2} (f \\circ g \\circ h)(w)$\n",
        "\n",
        "$\\frac{\\partial {out}}{\\partial{w_i}} |_{w_i=2} = (\\frac{1}{N}) f'(g(h(w))) \\cdot g'(h(w)) \\cdot h'(w) $\n",
        "\n",
        "$\\frac{\\partial {out}}{\\partial{w_i}} |_{w_i=2} = (\\frac{1}{N}) \\frac{1}{2\\sqrt{(\\frac{1}{3}w)^3}} \\cdot 3(\\frac{1}{3}w)^2 \\cdot \\frac{1}{3}$\n",
        "\n",
        "$\\frac{\\partial {out}}{\\partial{w_i}} |_{w_i=2} = (\\frac{1}{8}) \\frac{1}{2\\sqrt{(\\frac{1}{3}2)^3}} \\cdot 3(\\frac{1}{3}2)^2 \\cdot \\frac{1}{3}$\n",
        "\n",
        "$\\frac{\\partial {out}}{\\partial{w_i}} |_{w_i=2} = (\\frac{1}{8}) \\frac{3\\sqrt{6}}{8} \\cdot \\frac{4}{3} \\cdot \\frac{1}{3} = \\frac{\\sqrt{6}}{48} \\approx 0.05103$\n",
        "\n",
        "The calculation agrees with the program output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDLi1Fe6SNmk",
        "colab_type": "text"
      },
      "source": [
        "**Problem 6**\n",
        "\n",
        "Part (a)\n",
        "\n",
        "$\\hspace{1cm}$ The training set has $25$ data points. I would expect Adaline to to achieve a relatively high amount of error on this dataset because it is not linearly separable with a single perceptron.\n",
        "\n",
        "---\n",
        "\n",
        "Part (b)\n",
        "\n",
        "$\\hspace{1cm}$ When the train.dat data was trained on the Adaline model with only one neuron, an accuracy of $56\\%$ was achieved. This accuracy agrees with my answer from part (a).\n",
        "\n",
        "---\n",
        "\n",
        "Part (c)\n",
        "\n",
        "$\\hspace{1cm}$ tanh(h) is a good choice for the dataset because it makes $E_{in}$ differentiable and the optimizer is able to use stochastic gradient descent in order to find a local minimum.\n",
        "\n",
        "---\n",
        "\n",
        "Part (d)\n",
        "\n",
        "$\\hspace{1cm}$ When training the model for $600$ iterations on the training dataset with $\\eta=0.025$, the loss drops relatively quickly for the first $120$ epochs $(loss=0.97902 \\rightarrow loss=0.01008)$, then slowly reaches a loss of $0.00025$ by epoch $600$. I expected the training to converge to a reasonable error given that the learning rate was near $1.0$ and $600$ steps should be enough for a dataset of 25 points. The final training error was $0.00\\%$. For the test dataset, the final test error was $3.60\\%$. The test dataset had 250 data points, but assuming that $E_{in} \\approx E_{out}$, the $3.60\\%$ error rate demonstrates that a reasonable hypothesis was selected.\n",
        "\n",
        "---\n",
        "\n",
        "Part (e)\n",
        "\n",
        "$\\hspace{1cm}$ When training the model for $200$ iterations on the test dataset with $\\eta=0.0025$, the loss dropped relatively quickly at the beginning and converged to near zero. The error obtained for the training on the test.dat set was $3.20\\%$. The learned weights gave a $0.00\\%$ error on the train.dat dataset. My original thought was that the larger sample size would produce a good hypothesis, but I thought that the smaller $\\eta$ and number of epochs would slow it down, so I was surprised to see that the final error rate was comparable to part (d).\n",
        "\n",
        "---\n",
        "\n",
        "Part (f)\n",
        "\n",
        "$\\hspace{1cm}$ The train.dat data set had $25$ data points and the test.dat dataset had $250$ data points. As stated in part (e), training on a larger data set reduces the probability of error due to the $2e^{-2 \\epsilon^2 N}$ component, where the error bound decreases exponentially as $N$ increases.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "viE60z-HV6We",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sign(value):\n",
        "    if value >= 0.:\n",
        "        return 1.\n",
        "    else:\n",
        "        return -1.\n",
        "vsign = np.vectorize(sign)\n",
        "\n",
        "def mse(pred, target):\n",
        "    return (pred - target)**2\n",
        "\n",
        "def print_perf(model, data, labels):\n",
        "    \"\"\" Prints the prediction accuracy and error.\n",
        "    data: a training or test dataset.\n",
        "    labels: training or test labels.\n",
        "    \"\"\"\n",
        "    y_hat = model.predict(Variable(torch.Tensor([[[data]]])))\n",
        "    num_correct = ((y_hat.squeeze().reshape(-1,1) - labels) == 0.).sum()\n",
        "    num_total = data.shape[0]\n",
        "    accuracy = num_correct / num_total * 100.\n",
        "    error = 100. - accuracy\n",
        "    print(f'num_correct: {num_correct}   num_total: {num_total}')\n",
        "    print(f'Accuracy = {accuracy: .2f}%, error = {error: .2f}%')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yy1iSzpEI7B9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set up constants\n",
        "d = 2  # The dimensionality\n",
        "SEED = 7 # For reproducibility....\n",
        "np.random.seed(SEED)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tox5y3xhWimz",
        "colab_type": "text"
      },
      "source": [
        "Parse the training data set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYSJas3keqbh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set up empty vectors for capturing data from train.dat\n",
        "x0_train = []\n",
        "x1_train = []\n",
        "y_train = []\n",
        "\n",
        "# Open the mapped Google drive and parse the training data\n",
        "#with open('train.dat', 'r') as f:\n",
        "with open('test.dat', 'r') as f:\n",
        "    data = f.readlines()\n",
        "    for N, i in enumerate(data):\n",
        "        row = i.rstrip().lstrip().split(\",\")\n",
        "        for j in row:\n",
        "            num = j.rstrip().lstrip().split(\"  \")\n",
        "            x0_train.append(float(num[0]))\n",
        "            x1_train.append(float(num[1]))\n",
        "            y_train.append(float(num[2]))\n",
        "\n",
        "N += 1 # Adjust for the offset starting at 0\n",
        "\n",
        "# Join the x inputs into a matrix\n",
        "X_train = np.vstack((x0_train, x1_train))\n",
        "X_train = np.vstack((np.ones((1, N)), X_train))  # N points of (d + 1) dimensions, with 1st dim = 1.\n",
        "y_train = np.asarray(y_train).reshape(N, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAUZmoC4Eu5v",
        "colab_type": "code",
        "outputId": "5b1a97aa-b21b-4d54-b12d-e5e272193cdb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "source": [
        "# Categorize the data\n",
        "X_plus = np.asarray([X_train.T[row] for row in range(X_train.T.shape[0]) if y_train[row] > 0])\n",
        "X_minus = np.asarray([X_train.T[row] for row in range(X_train.T.shape[0]) if y_train[row] < 0])\n",
        "\n",
        "# Setting up the matplotlib plot....\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(X_plus[:, [1]], X_plus[:, [2]], 'ro', label='X Plus')\n",
        "ax.plot(X_minus[:, [1]], X_minus[:, [2]], 'bo', label='X Minus')\n",
        "ax.set_xlim(-1.1, 1.1)\n",
        "ax.set_ylim(-1.1, 1.1)\n",
        "ax.set_title(\"The Training Data\")\n",
        "_ = ax.legend()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEICAYAAABS0fM3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEBiGYdQcMwSGYRg1xwyBYRhGzfn/ZLwzJAKddgAAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLcZwneVVlYx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(2, 2)\n",
        "        self.fc2 = nn.Linear(2, 1)\n",
        "        #self.fc1.weight.data.fill_(0.0)  # Weights initialized to zero for Adaline!\n",
        "        #self.fc2.weight.data.fill_(0.0)\n",
        "        self.fc1.bias.data.fill_(0.0)\n",
        "        self.fc2.bias.data.fill_(0.0)\n",
        "        self.theta = nn.Tanh()\n",
        "\n",
        "    def forward(self, x):\n",
        "        s = self.theta(self.fc1(x))\n",
        "        return self.theta(self.fc2(s))\n",
        "\n",
        "    def _sign(self, x):\n",
        "        return 1. if x >= 0 else -1.\n",
        "        vsign = np.vectorize(self._sign)  # vectorize the function so it can apply elementwise op to array and return array\n",
        "\n",
        "    def predict(self, x):\n",
        "        return vsign(self.forward(x).detach())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsPAkV7sVmwL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ETA = 0.0025\n",
        "X_coord = X_train[1:3, :] # (2, N), stack of N training column vectors\n",
        "inputs_train = X_coord.T\n",
        "adaline = Net()\n",
        "optimizer = optim.SGD(adaline.parameters(), lr=ETA, momentum=0.3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TeUGH8_WnXC",
        "colab_type": "text"
      },
      "source": [
        "Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FwQozISWFRs",
        "colab_type": "code",
        "outputId": "deeb576a-bf9d-4218-dce0-5b857282dc03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        }
      },
      "source": [
        "NUM_ITERATIONS = 200\n",
        "updates = 0\n",
        "for epoch in range(NUM_ITERATIONS):\n",
        "    for i, coord in enumerate(inputs_train):\n",
        "        datapoint = Variable(torch.FloatTensor([coord]), requires_grad=True)\n",
        "        label = Variable(torch.FloatTensor([y_train[i]]), requires_grad=False)\n",
        "        optimizer.zero_grad()\n",
        "        out = adaline(datapoint)\n",
        "        loss = mse(out, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        updates += 1\n",
        "    if epoch % (NUM_ITERATIONS / 10) == 0:\n",
        "        print(f'Epoch {epoch}, loss = {loss.data[0].item():2.5f}')\n",
        "print(f'Epoch {epoch + 1}, loss = {loss.data[0].item():2.5f}')\n",
        "print(f'Total updates = {updates}')\n",
        "print(f'Parameter values = {list(adaline.parameters())}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0, loss = 1.16864\n",
            "Epoch 20, loss = 1.09801\n",
            "Epoch 40, loss = 0.16440\n",
            "Epoch 60, loss = 0.01470\n",
            "Epoch 80, loss = 0.00374\n",
            "Epoch 100, loss = 0.00148\n",
            "Epoch 120, loss = 0.00074\n",
            "Epoch 140, loss = 0.00043\n",
            "Epoch 160, loss = 0.00028\n",
            "Epoch 180, loss = 0.00020\n",
            "Epoch 200, loss = 0.00015\n",
            "Total updates = 50000\n",
            "Parameter values = [Parameter containing:\n",
            "tensor([[ 2.8656, -2.8921],\n",
            "        [ 2.8957, -2.7122]], requires_grad=True), Parameter containing:\n",
            "tensor([ 0.8634, -1.6708], requires_grad=True), Parameter containing:\n",
            "tensor([[-2.9604,  3.5561]], requires_grad=True), Parameter containing:\n",
            "tensor([2.6135], requires_grad=True)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svmFXEyEWPb-",
        "colab_type": "code",
        "outputId": "76662c28-b75b-4ce7-9be6-7e01e09615ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        }
      },
      "source": [
        "weights = adaline.fc1.weight.detach().numpy()\n",
        "bias = adaline.fc1.bias.detach().numpy()\n",
        "learned_w = np.append(bias, weights).reshape(3,2)\n",
        "print(f'Adaline learned the weights\\n {learned_w}')\n",
        "print_perf(adaline, inputs_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Adaline learned the weights\n",
            " [[ 0.86337847 -1.6707793 ]\n",
            " [ 2.8655713  -2.8920937 ]\n",
            " [ 2.895709   -2.7121668 ]]\n",
            "num_correct: 242   num_total: 250\n",
            "Accuracy =  96.80%, error =  3.20%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oou26d9J9jh2",
        "colab_type": "text"
      },
      "source": [
        "Parse and analyze the test data set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IE_GXbMoOHp5",
        "colab_type": "code",
        "outputId": "91288087-ccab-44ae-8598-215e2da762f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Set up empty vectors for capturing data from train.dat\n",
        "x0_test = []\n",
        "x1_test = []\n",
        "y_test = []\n",
        "\n",
        "# Open the mapped Google drive and parse the training data\n",
        "#with open('test.dat', 'r') as f:\n",
        "with open('train.dat', 'r') as f:\n",
        "    data = f.readlines()\n",
        "    for N, i in enumerate(data):\n",
        "        row = i.rstrip().lstrip().split(\",\")\n",
        "        for j in row:\n",
        "            num = j.rstrip().lstrip().split(\"  \")\n",
        "            x0_test.append(float(num[0]))\n",
        "            x1_test.append(float(num[1]))\n",
        "            y_test.append(float(num[2]))\n",
        "\n",
        "N += 1 # Adjust for the offset starting at 0\n",
        "\n",
        "# Join the x inputs into a matrix\n",
        "X_test = np.vstack((x0_test, x1_test))\n",
        "X_test = np.vstack((np.ones((1, N)), X_test))  # N points of (d + 1) dimensions, with 1st dim = 1.\n",
        "\n",
        "# Convert to numpy arrays\n",
        "y_test = np.asarray(y_test).reshape(N, 1)\n",
        "X_coord = X_test[1:3, :] # (2, N), stack of N test column vectors\n",
        "inputs_test = X_coord.T\n",
        "print_perf(adaline, inputs_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "num_correct: 25   num_total: 25\n",
            "Accuracy =  100.00%, error =  0.00%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}